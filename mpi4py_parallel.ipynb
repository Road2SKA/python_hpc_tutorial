{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kVWqN3EhJFbJ",
        "LJ_ThE3-KthZ",
        "Vww7AO93S8QD",
        "bXSiO5dGuJda",
        "49L8NojO6axL",
        "XxKI85d8LLll",
        "UKgBEDizMkRM",
        "lFx0VyZzN1Aw",
        "eJQfgyqlOTCr",
        "o6bvSqgMPeyt",
        "57wzDFu2xxvW",
        "n3eZ1sN1yVsK",
        "tNe1kQLpyqh2",
        "FcckAX8G0Pd-",
        "ocfu_ZYw3QCz",
        "0gcBP4q93eCj",
        "IGWlHIPh4t08",
        "cwiw83d44_gR",
        "PrwM90WO6jKv",
        "YRLLMm9j7iOA",
        "JnO-S8V37tpD",
        "P74yRNds9SBx",
        "gL2CQgIo-xTd",
        "1g5HzFU1--zY",
        "H9EYqJldBRS-",
        "tIw9YI6GB5go",
        "mPVwi5W4DpwJ",
        "-eCGJi2hEOC1",
        "qKhQXI7oFRMf",
        "t65YHXekHbBK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Road2SKA/python_hpc_tutorial/blob/main/mpi4py_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMYxZtKyP0I"
      },
      "source": [
        "# Distributed Parallel Programming Patterns using mpi4py\n",
        "\n",
        "The message passing interface (MPI) is a bottom-up approach to parallel programming. Message passing can be used on a single multicore computer or with a cluster of computers. The implementation of the message passing functions are based on original work by Joel Adams:\n",
        "\n",
        "Adams, Joel C. \"Patternlets: A Teaching Tool for Introducing Students to Parallel Design Patterns.\" 2015 IEEE International Parallel and Distributed Processing Symposium Workshop. IEEE, 2015.\n",
        "\n",
        "To run these examples, first you will need to install the mpi4py library by running this code (this will usually take a while to install the first time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVgDVtdkxrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa0fb11-452e-4d83-e420-32d4328219a7"
      },
      "source": [
        "! pip install mpi4py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n",
            "Downloading mpi4py-4.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqLzdrkgv2rw"
      },
      "source": [
        "**Important:** you will have to re-run this cell after you you get disconnected for a fairly long time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWhWJMLzUIm"
      },
      "source": [
        "## Simple example\n",
        "\n",
        "This code forms the basis of all of the other examples that follow. It is the fundamental way we structure parallel programs today.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhcbZEkzIAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41455b3d-dc50-4ae8-99c2-cf73c5707299"
      },
      "source": [
        "%%writefile my_program.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "    print(\"Greetings from process {} of {} on {}\"\\\n",
        "    .format(id, numProcesses, myHostName))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_program.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLFiYTIJ6ALs"
      },
      "source": [
        "Let's examine the variables created in lines 5-8 carefully.\n",
        "\n",
        "1. *comm* The fundamental notion with this type of computing is a *process* running independently on the computer. With one single program like this, we can specify that we want to start several processes, each of which can **communicate**. The mechanism for communication is initialized when the program starts up, and the object that represents the means of using communication between processes is called MPI.COMM_WORLD, which we place in the variable comm.\n",
        "\n",
        "2. *id* Every process can identify itself with a number. We get that number by asking *comm* for it using Get_rank().\n",
        "\n",
        "3. *numProcesses* It is helpful to know haw many processes have started up, because this can be specified differently every time you run this type of program. Asking *comm* for it is done with Get_size().\n",
        "\n",
        "4. *myHostName* When you run this code on a cluster of computers, it is sometimes useful to know which computer is running a certain piece of code. A particular computer is often called a 'host', which is why we call this variable myHostName, and get it by asking *comm* to provide it with Get_processor_name().\n",
        "\n",
        "These four variables are often used in every MPI program. The first three are often needed for writing correct programs, and the fourth one is often used for debugging and analysis of where certain computations are running.\n",
        "\n",
        "Next we see how we can use the mpirun program to execute the above python code using 4 processes. The value after -np is the number of processes to use when running the file of python code saved when executing the previous code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqOAtb4G4-e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af1e09e-689f-447a-ffd5-c487b69dacd8"
      },
      "source": [
        "! mpirun --allow-run-as-root --oversubscribe -np 8 python my_program.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings from process 2 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 6 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 5 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 0 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 4 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 1 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 3 of 8 on 9cf7bbd8e3da\n",
            "Greetings from process 7 of 8 on 9cf7bbd8e3da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR2tfQ8v8RVa"
      },
      "source": [
        "The fundamental idea of message passing programs can be illustrated like this:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1wpQaFiaubIcQBV9Lw_jwOU0y2-K-EChW)\n",
        "\n",
        "Each process is set up within a communication network to be able to communicate with every other process via communication links. Each process is set up to have its own number, or id, which starts at 0.\n",
        "\n",
        "**Note:** Each process holds its own copies of the above 4 data variables. **So even though there is one single program, it is running multiple times in separate processes, each holding its own data values.** The print line at the end of main() represents the multiple different data output produced by each process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSEhP3vv-_yX"
      },
      "source": [
        "## Prime numbers\n",
        "Let's try to implement the prime number function using MPI. Remember that our default serial implementation is:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    n_sqrt = math.floor(math.sqrt(n)) + 1\n",
        "    for i in range(2, n_sqrt):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "n = 6783858998837822\n",
        "%time res=is_prime(n)\n",
        "print(\"{0} prime: {1}\".format(n,res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkWxk_cM_3DM",
        "outputId": "615c1437-e73c-4345-c6b4-d11aedfd726f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
            "Wall time: 15.7 µs\n",
            "6783858998837822 prime: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to implement this using MPI:"
      ],
      "metadata": {
        "id": "Qnegx5CZAeHa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlF-_TYc_uKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8876a40b-6984-4dd8-eb80-74157ded3e9e"
      },
      "source": [
        "%%writefile mpi_primes.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import math\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "\n",
        "    print(\"Greetings task {} of {} on {}\".format(id, numProcesses, myHostName))\n",
        "\n",
        "    # here is our input. We could also read this from the command line instead of hard-coding the value\n",
        "    n = 6783858998837822\n",
        "\n",
        "    is_prime = True # our default guess.\n",
        "    if n <= 1:\n",
        "      is_prime = False\n",
        "    else:\n",
        "      n_sqrt = math.floor(math.sqrt(n)) + 1\n",
        "      # split up this loop over the different processes.\n",
        "      check_nums = np.split(np.arange(2,n_sqrt),numProcesses)\n",
        "      check_nums = check_nums[id]\n",
        "\n",
        "      for i in check_nums:\n",
        "        if n % i == 0:\n",
        "            is_prime = False\n",
        "    print(\"task {} finds {}\".format(id, is_prime))\n",
        "\n",
        "########## Run the main function\n",
        "main()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_primes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8HeRMx-APS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49052163-8ab9-4181-c9c2-b4343214146a"
      },
      "source": [
        "! mpirun --allow-run-as-root --oversubscribe -np 7 python mpi_primes.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings task 4 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 2 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 3 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 5 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 6 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 1 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 0 of 7 on 9cf7bbd8e3da\n",
            "task 3 finds True\n",
            "task 5 finds True\n",
            "task 0 finds False\n",
            "task 1 finds True\n",
            "task 4 finds True\n",
            "task 2 finds True\n",
            "task 6 finds True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to modify the code so that we only get 1 result instead of 7. We'll use one process, process 0, as our special \"conductor\" process which collects all of the results of the others. To do this we will need to use communication, the \"message passing\" of MPI.\n",
        "\n",
        "There are many different ways to pass mssages:\n",
        "point-to-point, broadcasting, scatter/gather, etc. You can find information on all of the different patterns at [https://mpi4py.readthedocs.io](https://mpi4py.readthedocs.io).\n",
        "\n",
        "Here, we will"
      ],
      "metadata": {
        "id": "X6_PNsVnAnb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_primes2.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import math\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD\n",
        "    id = comm.Get_rank()            #number of the process running the code\n",
        "    numProcesses = comm.Get_size()  #total number of processes running\n",
        "    myHostName = MPI.Get_processor_name()  #machine name running the code\n",
        "\n",
        "\n",
        "    print(\"Greetings task {} of {} on {}\".format(id, numProcesses, myHostName))\n",
        "\n",
        "    # here is our input. We could also read this from the command line instead of hard-coding the value\n",
        "    n = 6783858998837822\n",
        "\n",
        "    is_prime = True # our default guess.\n",
        "    if n <= 1:\n",
        "      is_prime = False\n",
        "\n",
        "    # task 0 will define the loop and send it to the other processes using SCATTER\n",
        "    if id == 0:\n",
        "      n_sqrt = math.floor(math.sqrt(n)) + 1\n",
        "      # split up this loop over the different processes.\n",
        "      check_nums = np.split(np.arange(2,n_sqrt),numProcesses)\n",
        "    else:\n",
        "      check_nums = None\n",
        "    check_nums = comm.scatter(check_nums, root=0)\n",
        "\n",
        "    print(\"task {} received {}\".format(id, check_nums))\n",
        "\n",
        "    # now each task checks its part of the list\n",
        "    for i in check_nums:\n",
        "        if n % i == 0:\n",
        "            is_prime = False\n",
        "\n",
        "    # task 0 will collect all of the work done by other processes using GATHER\n",
        "    result = comm.gather(is_prime, root=0)\n",
        "    if id == 0:\n",
        "      print(\"task {} received {}\".format(id, result))\n",
        "      is_prime = not (False in result)\n",
        "      print(\"{} prime: {}\".format(n, is_prime))\n",
        "\n",
        "    else:\n",
        "      assert result is None\n",
        "\n",
        "\n",
        "########## Run the main function\n",
        "main()"
      ],
      "metadata": {
        "id": "4I0HBlm4A_zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a3ad98-5938-4b73-c194-cd7364efdd66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_primes2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root --oversubscribe -np 7 python mpi_primes2.py"
      ],
      "metadata": {
        "id": "-qVgxg5dCrbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70914d19-2067-453c-dcf2-b2fce447db43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greetings task 2 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 3 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 4 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 1 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 6 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 5 of 7 on 9cf7bbd8e3da\n",
            "Greetings task 0 of 7 on 9cf7bbd8e3da\n",
            "task 1 received [11766314 11766315 11766316 ... 23532623 23532624 23532625]\n",
            "task 2 received [23532626 23532627 23532628 ... 35298935 35298936 35298937]\n",
            "task 3 received [35298938 35298939 35298940 ... 47065247 47065248 47065249]\n",
            "task 5 received [58831562 58831563 58831564 ... 70597871 70597872 70597873]\n",
            "task 4 received [47065250 47065251 47065252 ... 58831559 58831560 58831561]\n",
            "task 0 received [       2        3        4 ... 11766311 11766312 11766313]\n",
            "task 6 received [70597874 70597875 70597876 ... 82364183 82364184 82364185]\n",
            "task 0 received [False, True, True, True, True, True, True]\n",
            "6783858998837822 prime: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the different tasks and communication between the different processes means that MPI will be slower than single-process execution with vectorization. You will see the best performance when running across many nodes on large datasets."
      ],
      "metadata": {
        "id": "uuAcDjSqDbqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eyj6sa7GoXu"
      },
      "source": [
        "**Exercise:** rerun the prime-funder script, using varying numbers of processes from 1 through 8 (i.e., vary the argument after -np). Explain what stays the same and what changes as the number of processes changes."
      ]
    }
  ]
}